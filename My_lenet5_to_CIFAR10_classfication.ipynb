{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "My lenet5 to CIFAR10 classfication.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNLGV67hKH83Gx/m3kQVkeQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c67afd65e3b1460591a68b4a86e12e57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b378a8858cac42dd96c3550012726ecc",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2da36b26343a4a88a49a9225544841d6",
              "IPY_MODEL_06e49f9ca12347f8b60f802e4bad9628"
            ]
          }
        },
        "b378a8858cac42dd96c3550012726ecc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2da36b26343a4a88a49a9225544841d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9c6878b310844478a123049a1a7e7fe0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3a7f17db0b3a42399b30c36c4f2e2022"
          }
        },
        "06e49f9ca12347f8b60f802e4bad9628": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bba53e3ae85a4f58bf821ad02f16e22f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [00:20&lt;00:00, 82185866.93it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4b693fd8f70242549671c8eef89b3821"
          }
        },
        "9c6878b310844478a123049a1a7e7fe0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3a7f17db0b3a42399b30c36c4f2e2022": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bba53e3ae85a4f58bf821ad02f16e22f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4b693fd8f70242549671c8eef89b3821": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QI-N-QIGT/lenet5/blob/master/My_lenet5_to_CIFAR10_classfication.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4kHND5-Hxfa"
      },
      "source": [
        "### Install some necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWo-2STIH7uZ"
      },
      "source": [
        "pip install hiddenlayer  # a kind of visualable tool"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S17ioheKkEiX"
      },
      "source": [
        "### The first part is to download the cifar10 dataset and process it. We can download it from torchvision."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdtD-r-1kDNf"
      },
      "source": [
        "import torch\r\n",
        "import torchvision\r\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53TIWU14osHx",
        "outputId": "a63b7059-af61-4dfe-88db-8b6c70e6d662"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "# detect whether the GPU is existed.\r\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BXKuq4UvDmo"
      },
      "source": [
        "import numpy as np\r\n",
        "# load data\r\n",
        "from torchvision import datasets\r\n",
        "# load the training data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jh7_1yl8zJsk"
      },
      "source": [
        "### Calculate the mean and std of CIFAR10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYKawhvbvV3j",
        "outputId": "96c26d88-2efd-4145-99f4-2edb9583a16d"
      },
      "source": [
        "train_data = datasets.CIFAR10('./cifar10_data', train=True, download=True)\r\n",
        "test_data = datasets.CIFAR10('./cifar10_data_test', train=False, download=True)\r\n",
        "print(f\"lenght of test_data is: {len(test_data)}\")\r\n",
        "print(f\"lenght of train_data is: {len(train_data)}\")\r\n",
        "print(np.asarray(train_data[1][0]).shape)\r\n",
        "\r\n",
        "# use np.concatenate to stick all the images together to form a 1600000(50000*32) X 32 X 3 array\r\n",
        "x = np.concatenate([np.asarray(train_data[i][0]) for i in range(len(train_data))])\r\n",
        "# print(x)\r\n",
        "print(x.shape)\r\n",
        "# calculate the mean and std along the (0, 1) axes\r\n",
        "train_mean = np.mean(x, axis=(0, 1)) / 255\r\n",
        "train_std = np.std(x, axis=(0, 1)) /255\r\n",
        "# the the mean and std\r\n",
        "print(train_mean, train_std)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1600000, 32, 3)\n",
            "[0.49139968 0.48215841 0.44653091] [0.24703223 0.24348513 0.26158784]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBq_hyqz4UUm"
      },
      "source": [
        "### Preprocess the data\r\n",
        "1. Converts a PIL.Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0].\r\n",
        "2. tansforms.Normalize is to set the distribution into stand normal one\r\n",
        "3. transform.Compose is to compose the several tranform operations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102,
          "referenced_widgets": [
            "c67afd65e3b1460591a68b4a86e12e57",
            "b378a8858cac42dd96c3550012726ecc",
            "2da36b26343a4a88a49a9225544841d6",
            "06e49f9ca12347f8b60f802e4bad9628",
            "9c6878b310844478a123049a1a7e7fe0",
            "3a7f17db0b3a42399b30c36c4f2e2022",
            "bba53e3ae85a4f58bf821ad02f16e22f",
            "4b693fd8f70242549671c8eef89b3821"
          ]
        },
        "id": "fK0Be3TnoCUE",
        "outputId": "928ea6bf-27dd-46b1-9935-8b782c396c49"
      },
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.491, 0.482, 0.447], [0.247, 0.243, 0.262])])\r\n",
        "dataset_cifar10_train = torchvision.datasets.CIFAR10(root=\"./cifa10_data\", train = True, transform = transform, download = True)\r\n",
        "dataset_cifar10_test = torchvision.datasets.CIFAR10(root=\"./cifa10_data\", train = False, transform = transform, download = True)\r\n",
        "class_10_name = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifa10_data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c67afd65e3b1460591a68b4a86e12e57",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./cifa10_data/cifar-10-python.tar.gz to ./cifa10_data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_7pF57X7Owt"
      },
      "source": [
        "### We just need prepare the data for training using DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1w-Rb4wq4gi"
      },
      "source": [
        "from torch.utils.data import DataLoader\r\n",
        "batch_size = 50\r\n",
        "\r\n",
        "train_dataloader = DataLoader(dataset_cifar10_train, batch_size=batch_size, shuffle=True)\r\n",
        "test_dataloader = DataLoader(dataset_cifar10_test, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0Kag8skXpmX"
      },
      "source": [
        "### Display the figures to check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "10sPrfiV-4cg",
        "outputId": "9ae17ac5-0512-4fc0-cc28-868e86a8bf13"
      },
      "source": [
        "%matplotlib inline\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "# Display image and label.\r\n",
        "train_features, train_labels = next(iter(train_dataloader))\r\n",
        "print(f\"Feature batch shape: {train_features.size()}\")\r\n",
        "print(f\"Labels batch shape: {train_labels.size()}\")\r\n",
        "img = train_features[0].squeeze()\r\n",
        "img = torch.transpose(img, 0, 1)\r\n",
        "img = torch.transpose(img, 1, 2)\r\n",
        "print(img.shape)\r\n",
        "torch.transpose(img,1,2)\r\n",
        "label = train_labels[0]\r\n",
        "plt.imshow(img)\r\n",
        "print(f\"Label: {class_10_name[label]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Feature batch shape: torch.Size([50, 3, 32, 32])\n",
            "Labels batch shape: torch.Size([50])\n",
            "torch.Size([32, 32, 3])\n",
            "Label: car\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIemI2J3DmUE"
      },
      "source": [
        "### I think I have prepared the train data well, and then I will build the NN structure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEhvFzMHDjyf"
      },
      "source": [
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0N0tmvGPESqm"
      },
      "source": [
        "# we use the classical CNN that is lenet-5\r\n",
        "\r\n",
        "class Net(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "      super(Net, self).__init__()\r\n",
        "      self.conv1 = nn.Conv2d(3, 6, 5)\r\n",
        "      self.maxpool2 = nn.MaxPool2d(2,2)\r\n",
        "      self.conv3 = nn.Conv2d(6,16,5)\r\n",
        "      self.maxpool4 = nn.MaxPool2d(2,2)\r\n",
        "      self.dense5 = nn.Linear(16*5*5,120) # self.conv5 = nn.Linear(16,120,5) is OK. \r\n",
        "      self.dense6 = nn.Linear(120,84)\r\n",
        "      self.dense7 = nn.Linear(84,10)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "      x = F.relu(self.conv1(x))\r\n",
        "      x = self.maxpool2(x)\r\n",
        "      x = F.relu(self.conv3(x))\r\n",
        "      x = self.maxpool4(x)        # the dimmentions of x is [batch_size, 16, 5, 5]\r\n",
        "      x = x.view(-1, 16*5*5)      # this command is used to compose the conv layer and linear layer,and the second aixs num is 16*5*5\r\n",
        "      x = F.relu(self.dense5(x))  # the dimmentions of x is [16*5*5,120]\r\n",
        "      x = F.relu(self.dense6(x))\r\n",
        "      x = self.dense7(x)\r\n",
        "      return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmFAXULhjflE"
      },
      "source": [
        "### OK, I think I have bulit the lenet5 structure. Then I will begain train the neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uZComJCj3fi"
      },
      "source": [
        "lenet5_net = Net()\r\n",
        "lenet5_net = lenet5_net.to(device)\r\n",
        "print(lenet5_net)\r\n",
        "# we have to define the loss function and optimization\r\n",
        "loss_fun = nn.CrossEntropyLoss()\r\n",
        "opt = torch.optim.SGD(lenet5_net.parameters(), lr = 0.001, momentum = 0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R19_f7rt6NU2"
      },
      "source": [
        "### This is a more complete function with loss and accuracy output "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoGFp6HBYOhA"
      },
      "source": [
        "import time\r\n",
        "import hiddenlayer as hl\r\n",
        "\r\n",
        "# I am trying to add the calculation of accuracy into the code\r\n",
        "def train_process(model, epoch, loss_fun, optimizer, train_dataloader,test_dataloader):\r\n",
        "  best_acc = 0.0\r\n",
        "\r\n",
        "  for i in range(epoch):\r\n",
        "\r\n",
        "    print('Epoch {}/{}'.format(i+1, epoch))\r\n",
        "    print(\"*\" * 30)\r\n",
        "    # there are two phases in train processes\r\n",
        "    for modes in [\"train\",\"val\"]:\r\n",
        "      running_loss = 0\r\n",
        "      temp_loss = 0\r\n",
        "      temp_correction = 0\r\n",
        "      running_correction = 0\r\n",
        "      if modes is \"train\":\r\n",
        "        model.train() \r\n",
        "        for index, input_data in enumerate(train_dataloader, 0):\r\n",
        "          # get the images and speed up by cuda\r\n",
        "          imgs, labels = input_data\r\n",
        "          imgs, labels = imgs.to(device), labels.to(device)\r\n",
        "          # set the gradients all to zero\r\n",
        "          opt.zero_grad() \r\n",
        "\r\n",
        "          with torch.set_grad_enabled(modes == \"train\"):\r\n",
        "            # forwar & backward & optimization\r\n",
        "            outputs = model(imgs)\r\n",
        "            loss = loss_fun(outputs, labels)\r\n",
        "            loss.backward()\r\n",
        "            opt.step()\r\n",
        "          running_loss += loss.item()  # transfer a tensor with one element to a number\r\n",
        "          temp_loss += loss.item()\r\n",
        "          running_correction += (torch.max(outputs,1).indices == labels).sum().item()\r\n",
        "          temp_correction += (torch.max(outputs,1).indices == labels).sum().item()\r\n",
        "          epoch_loss = running_loss / len(train_dataloader)\r\n",
        "          epoch_acc = float(running_correction) / len(dataset_cifar10_train)\r\n",
        "          if (index+1) % 250 == 0:\r\n",
        "            print('[%d, %5d] train loss: %.3f' % (i + 1, index + 1, temp_loss / 250))\r\n",
        "            print('[%d, %5d] train acc: %.3f' % (i + 1, index + 1, temp_correction / (250*batch_size)))\r\n",
        "            temp_loss = 0\r\n",
        "            temp_correction = 0\r\n",
        "          # print(f\"{modes} Loss: {epoch_loss}, acc: {epoch_acc}\")\r\n",
        "\r\n",
        "      elif modes is \"val\":\r\n",
        "        model.eval()\r\n",
        "        for index, test_data in enumerate(test_dataloader, 0):\r\n",
        "          # get the images and speed up by cuda\r\n",
        "          imgs, labels = test_data\r\n",
        "          imgs, labels = imgs.to(device), labels.to(device) \r\n",
        "          # set the gradients all to zero\r\n",
        "          opt.zero_grad() \r\n",
        "          with torch.set_grad_enabled(modes == \"train\"):\r\n",
        "            # forward \r\n",
        "            outputs = model(imgs)\r\n",
        "            loss = loss_fun(outputs, labels)\r\n",
        "          running_loss += loss.item()  # transfer a tensor with one element to a number\r\n",
        "          temp_loss += loss.item()  # transfer a tensor with one element to a number\r\n",
        "          running_correction += (torch.max(outputs,1).indices == labels).sum().item()\r\n",
        "          temp_correction += (torch.max(outputs,1).indices == labels).sum().item()\r\n",
        "          epoch_loss = running_loss / len(test_dataloader)\r\n",
        "          epoch_acc = float(running_correction) / len(dataset_cifar10_test)\r\n",
        "          if (index+1) % 50 == 0:\r\n",
        "            print('[%d, %5d] train loss: %.3f' % (i + 1, index + 1, temp_loss / 50))\r\n",
        "            print('[%d, %5d] train acc: %.3f' % (i + 1, index + 1, temp_correction / (50*batch_size)))\r\n",
        "            temp_loss = 0\r\n",
        "            temp_correction = 0\r\n",
        "\r\n",
        "          #print(f\"{modes} Loss: {epoch_loss}, acc: {epoch_acc}\")\r\n",
        "          if epoch_acc > best_acc:\r\n",
        "            best_acc = epoch_acc\r\n",
        "\r\n",
        "\r\n",
        "      if modes is \"train\":\r\n",
        "        print(f\"{modes} Loss: {epoch_loss}, acc: {epoch_acc}\")\r\n",
        "      elif modes is \"val\":\r\n",
        "        print(f\"{modes} Loss: {epoch_loss}, acc: {epoch_acc}\")\r\n",
        "\r\n",
        "    print(\"*\"*30)\r\n",
        "\r\n",
        "  print(f\"best_acc is {best_acc}\")          \r\n",
        "  print(\"trainning finished\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzdgcI8CtBqp",
        "outputId": "5394ce0a-007a-4c29-c4c7-d58794afd9cf"
      },
      "source": [
        "train_process(model = lenet5_net, epoch=20, loss_fun=loss_fun, optimizer = opt, train_dataloader = train_dataloader,test_dataloader=test_dataloader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "******************************\n",
            "[1,   250] train loss: 0.375\n",
            "[1,   250] train acc: 0.866\n",
            "[1,   500] train loss: 0.395\n",
            "[1,   500] train acc: 0.857\n",
            "[1,   750] train loss: 0.424\n",
            "[1,   750] train acc: 0.844\n",
            "[1,  1000] train loss: 0.428\n",
            "[1,  1000] train acc: 0.848\n",
            "train Loss: 0.405366850450635, acc: 0.8538\n",
            "[1,    50] train loss: 1.506\n",
            "[1,    50] train acc: 0.636\n",
            "[1,   100] train loss: 1.500\n",
            "[1,   100] train acc: 0.634\n",
            "[1,   150] train loss: 1.526\n",
            "[1,   150] train acc: 0.608\n",
            "[1,   200] train loss: 1.490\n",
            "[1,   200] train acc: 0.634\n",
            "val Loss: 1.5053382363915444, acc: 0.6277\n",
            "******************************\n",
            "Epoch 2/20\n",
            "******************************\n",
            "[2,   250] train loss: 0.355\n",
            "[2,   250] train acc: 0.876\n",
            "[2,   500] train loss: 0.386\n",
            "[2,   500] train acc: 0.864\n",
            "[2,   750] train loss: 0.416\n",
            "[2,   750] train acc: 0.851\n",
            "[2,  1000] train loss: 0.423\n",
            "[2,  1000] train acc: 0.848\n",
            "train Loss: 0.3948166609406471, acc: 0.85958\n",
            "[2,    50] train loss: 1.460\n",
            "[2,    50] train acc: 0.637\n",
            "[2,   100] train loss: 1.529\n",
            "[2,   100] train acc: 0.629\n",
            "[2,   150] train loss: 1.553\n",
            "[2,   150] train acc: 0.623\n",
            "[2,   200] train loss: 1.470\n",
            "[2,   200] train acc: 0.647\n",
            "val Loss: 1.5030077674984932, acc: 0.6339\n",
            "******************************\n",
            "Epoch 3/20\n",
            "******************************\n",
            "[3,   250] train loss: 0.369\n",
            "[3,   250] train acc: 0.869\n",
            "[3,   500] train loss: 0.370\n",
            "[3,   500] train acc: 0.869\n",
            "[3,   750] train loss: 0.398\n",
            "[3,   750] train acc: 0.857\n",
            "[3,  1000] train loss: 0.393\n",
            "[3,  1000] train acc: 0.857\n",
            "train Loss: 0.3827077387645841, acc: 0.86326\n",
            "[3,    50] train loss: 1.578\n",
            "[3,    50] train acc: 0.622\n",
            "[3,   100] train loss: 1.597\n",
            "[3,   100] train acc: 0.626\n",
            "[3,   150] train loss: 1.525\n",
            "[3,   150] train acc: 0.630\n",
            "[3,   200] train loss: 1.501\n",
            "[3,   200] train acc: 0.640\n",
            "val Loss: 1.5500822228193283, acc: 0.6294\n",
            "******************************\n",
            "Epoch 4/20\n",
            "******************************\n",
            "[4,   250] train loss: 0.345\n",
            "[4,   250] train acc: 0.881\n",
            "[4,   500] train loss: 0.357\n",
            "[4,   500] train acc: 0.873\n",
            "[4,   750] train loss: 0.389\n",
            "[4,   750] train acc: 0.862\n",
            "[4,  1000] train loss: 0.415\n",
            "[4,  1000] train acc: 0.849\n",
            "train Loss: 0.3764254232272506, acc: 0.8662\n",
            "[4,    50] train loss: 1.573\n",
            "[4,    50] train acc: 0.611\n",
            "[4,   100] train loss: 1.463\n",
            "[4,   100] train acc: 0.634\n",
            "[4,   150] train loss: 1.552\n",
            "[4,   150] train acc: 0.627\n",
            "[4,   200] train loss: 1.569\n",
            "[4,   200] train acc: 0.622\n",
            "val Loss: 1.5393669274449349, acc: 0.6236\n",
            "******************************\n",
            "Epoch 5/20\n",
            "******************************\n",
            "[5,   250] train loss: 0.324\n",
            "[5,   250] train acc: 0.885\n",
            "[5,   500] train loss: 0.356\n",
            "[5,   500] train acc: 0.875\n",
            "[5,   750] train loss: 0.379\n",
            "[5,   750] train acc: 0.864\n",
            "[5,  1000] train loss: 0.395\n",
            "[5,  1000] train acc: 0.859\n",
            "train Loss: 0.3635612823739648, acc: 0.8707\n",
            "[5,    50] train loss: 1.547\n",
            "[5,    50] train acc: 0.634\n",
            "[5,   100] train loss: 1.529\n",
            "[5,   100] train acc: 0.627\n",
            "[5,   150] train loss: 1.624\n",
            "[5,   150] train acc: 0.625\n",
            "[5,   200] train loss: 1.630\n",
            "[5,   200] train acc: 0.618\n",
            "val Loss: 1.5822952517867088, acc: 0.6259\n",
            "******************************\n",
            "Epoch 6/20\n",
            "******************************\n",
            "[6,   250] train loss: 0.335\n",
            "[6,   250] train acc: 0.885\n",
            "[6,   500] train loss: 0.353\n",
            "[6,   500] train acc: 0.874\n",
            "[6,   750] train loss: 0.356\n",
            "[6,   750] train acc: 0.873\n",
            "[6,  1000] train loss: 0.384\n",
            "[6,  1000] train acc: 0.863\n",
            "train Loss: 0.35689899254590274, acc: 0.87376\n",
            "[6,    50] train loss: 1.703\n",
            "[6,    50] train acc: 0.624\n",
            "[6,   100] train loss: 1.621\n",
            "[6,   100] train acc: 0.623\n",
            "[6,   150] train loss: 1.694\n",
            "[6,   150] train acc: 0.603\n",
            "[6,   200] train loss: 1.622\n",
            "[6,   200] train acc: 0.636\n",
            "val Loss: 1.6601180946826934, acc: 0.6215\n",
            "******************************\n",
            "Epoch 7/20\n",
            "******************************\n",
            "[7,   250] train loss: 0.302\n",
            "[7,   250] train acc: 0.892\n",
            "[7,   500] train loss: 0.343\n",
            "[7,   500] train acc: 0.877\n",
            "[7,   750] train loss: 0.371\n",
            "[7,   750] train acc: 0.868\n",
            "[7,  1000] train loss: 0.370\n",
            "[7,  1000] train acc: 0.867\n",
            "train Loss: 0.34651728252321484, acc: 0.87592\n",
            "[7,    50] train loss: 1.674\n",
            "[7,    50] train acc: 0.627\n",
            "[7,   100] train loss: 1.682\n",
            "[7,   100] train acc: 0.621\n",
            "[7,   150] train loss: 1.686\n",
            "[7,   150] train acc: 0.614\n",
            "[7,   200] train loss: 1.591\n",
            "[7,   200] train acc: 0.628\n",
            "val Loss: 1.6581100377440452, acc: 0.6226\n",
            "******************************\n",
            "Epoch 8/20\n",
            "******************************\n",
            "[8,   250] train loss: 0.312\n",
            "[8,   250] train acc: 0.890\n",
            "[8,   500] train loss: 0.352\n",
            "[8,   500] train acc: 0.874\n",
            "[8,   750] train loss: 0.349\n",
            "[8,   750] train acc: 0.873\n",
            "[8,  1000] train loss: 0.377\n",
            "[8,  1000] train acc: 0.863\n",
            "train Loss: 0.347344906643033, acc: 0.87488\n",
            "[8,    50] train loss: 1.756\n",
            "[8,    50] train acc: 0.618\n",
            "[8,   100] train loss: 1.661\n",
            "[8,   100] train acc: 0.631\n",
            "[8,   150] train loss: 1.727\n",
            "[8,   150] train acc: 0.605\n",
            "[8,   200] train loss: 1.759\n",
            "[8,   200] train acc: 0.612\n",
            "val Loss: 1.7254538002610207, acc: 0.6166\n",
            "******************************\n",
            "Epoch 9/20\n",
            "******************************\n",
            "[9,   250] train loss: 0.299\n",
            "[9,   250] train acc: 0.895\n",
            "[9,   500] train loss: 0.324\n",
            "[9,   500] train acc: 0.883\n",
            "[9,   750] train loss: 0.347\n",
            "[9,   750] train acc: 0.877\n",
            "[9,  1000] train loss: 0.364\n",
            "[9,  1000] train acc: 0.867\n",
            "train Loss: 0.3334229108989239, acc: 0.88046\n",
            "[9,    50] train loss: 1.760\n",
            "[9,    50] train acc: 0.629\n",
            "[9,   100] train loss: 1.713\n",
            "[9,   100] train acc: 0.626\n",
            "[9,   150] train loss: 1.801\n",
            "[9,   150] train acc: 0.606\n",
            "[9,   200] train loss: 1.753\n",
            "[9,   200] train acc: 0.620\n",
            "val Loss: 1.7568468609452248, acc: 0.6204\n",
            "******************************\n",
            "Epoch 10/20\n",
            "******************************\n",
            "[10,   250] train loss: 0.301\n",
            "[10,   250] train acc: 0.896\n",
            "[10,   500] train loss: 0.318\n",
            "[10,   500] train acc: 0.886\n",
            "[10,   750] train loss: 0.339\n",
            "[10,   750] train acc: 0.879\n",
            "[10,  1000] train loss: 0.346\n",
            "[10,  1000] train acc: 0.872\n",
            "train Loss: 0.3260144548639655, acc: 0.88334\n",
            "[10,    50] train loss: 1.800\n",
            "[10,    50] train acc: 0.612\n",
            "[10,   100] train loss: 1.714\n",
            "[10,   100] train acc: 0.630\n",
            "[10,   150] train loss: 1.764\n",
            "[10,   150] train acc: 0.627\n",
            "[10,   200] train loss: 1.674\n",
            "[10,   200] train acc: 0.628\n",
            "val Loss: 1.7379126399755478, acc: 0.6241\n",
            "******************************\n",
            "Epoch 11/20\n",
            "******************************\n",
            "[11,   250] train loss: 0.292\n",
            "[11,   250] train acc: 0.896\n",
            "[11,   500] train loss: 0.306\n",
            "[11,   500] train acc: 0.890\n",
            "[11,   750] train loss: 0.317\n",
            "[11,   750] train acc: 0.889\n",
            "[11,  1000] train loss: 0.349\n",
            "[11,  1000] train acc: 0.872\n",
            "train Loss: 0.316283470928669, acc: 0.88642\n",
            "[11,    50] train loss: 1.793\n",
            "[11,    50] train acc: 0.618\n",
            "[11,   100] train loss: 1.775\n",
            "[11,   100] train acc: 0.619\n",
            "[11,   150] train loss: 1.793\n",
            "[11,   150] train acc: 0.618\n",
            "[11,   200] train loss: 1.798\n",
            "[11,   200] train acc: 0.611\n",
            "val Loss: 1.7898191753029824, acc: 0.6163\n",
            "******************************\n",
            "Epoch 12/20\n",
            "******************************\n",
            "[12,   250] train loss: 0.273\n",
            "[12,   250] train acc: 0.906\n",
            "[12,   500] train loss: 0.311\n",
            "[12,   500] train acc: 0.888\n",
            "[12,   750] train loss: 0.319\n",
            "[12,   750] train acc: 0.884\n",
            "[12,  1000] train loss: 0.335\n",
            "[12,  1000] train acc: 0.878\n",
            "train Loss: 0.309541654676199, acc: 0.8891\n",
            "[12,    50] train loss: 1.853\n",
            "[12,    50] train acc: 0.611\n",
            "[12,   100] train loss: 1.829\n",
            "[12,   100] train acc: 0.619\n",
            "[12,   150] train loss: 1.947\n",
            "[12,   150] train acc: 0.614\n",
            "[12,   200] train loss: 1.936\n",
            "[12,   200] train acc: 0.614\n",
            "val Loss: 1.8912799811363221, acc: 0.6145\n",
            "******************************\n",
            "Epoch 13/20\n",
            "******************************\n",
            "[13,   250] train loss: 0.262\n",
            "[13,   250] train acc: 0.908\n",
            "[13,   500] train loss: 0.285\n",
            "[13,   500] train acc: 0.900\n",
            "[13,   750] train loss: 0.335\n",
            "[13,   750] train acc: 0.877\n",
            "[13,  1000] train loss: 0.334\n",
            "[13,  1000] train acc: 0.877\n",
            "train Loss: 0.30393104305863383, acc: 0.89062\n",
            "[13,    50] train loss: 1.789\n",
            "[13,    50] train acc: 0.624\n",
            "[13,   100] train loss: 1.867\n",
            "[13,   100] train acc: 0.620\n",
            "[13,   150] train loss: 1.791\n",
            "[13,   150] train acc: 0.618\n",
            "[13,   200] train loss: 1.823\n",
            "[13,   200] train acc: 0.623\n",
            "val Loss: 1.817370181083679, acc: 0.6212\n",
            "******************************\n",
            "Epoch 14/20\n",
            "******************************\n",
            "[14,   250] train loss: 0.253\n",
            "[14,   250] train acc: 0.911\n",
            "[14,   500] train loss: 0.283\n",
            "[14,   500] train acc: 0.900\n",
            "[14,   750] train loss: 0.312\n",
            "[14,   750] train acc: 0.888\n",
            "[14,  1000] train loss: 0.328\n",
            "[14,  1000] train acc: 0.880\n",
            "train Loss: 0.2937837238535285, acc: 0.89474\n",
            "[14,    50] train loss: 1.968\n",
            "[14,    50] train acc: 0.619\n",
            "[14,   100] train loss: 1.904\n",
            "[14,   100] train acc: 0.627\n",
            "[14,   150] train loss: 1.889\n",
            "[14,   150] train acc: 0.611\n",
            "[14,   200] train loss: 1.993\n",
            "[14,   200] train acc: 0.617\n",
            "val Loss: 1.9384158796072006, acc: 0.6184\n",
            "******************************\n",
            "Epoch 15/20\n",
            "******************************\n",
            "[15,   250] train loss: 0.264\n",
            "[15,   250] train acc: 0.907\n",
            "[15,   500] train loss: 0.292\n",
            "[15,   500] train acc: 0.892\n",
            "[15,   750] train loss: 0.303\n",
            "[15,   750] train acc: 0.891\n",
            "[15,  1000] train loss: 0.323\n",
            "[15,  1000] train acc: 0.886\n",
            "train Loss: 0.29541150147467854, acc: 0.89374\n",
            "[15,    50] train loss: 2.003\n",
            "[15,    50] train acc: 0.614\n",
            "[15,   100] train loss: 1.927\n",
            "[15,   100] train acc: 0.624\n",
            "[15,   150] train loss: 2.088\n",
            "[15,   150] train acc: 0.605\n",
            "[15,   200] train loss: 1.902\n",
            "[15,   200] train acc: 0.633\n",
            "val Loss: 1.9798093900084495, acc: 0.6192\n",
            "******************************\n",
            "Epoch 16/20\n",
            "******************************\n",
            "[16,   250] train loss: 0.262\n",
            "[16,   250] train acc: 0.909\n",
            "[16,   500] train loss: 0.280\n",
            "[16,   500] train acc: 0.897\n",
            "[16,   750] train loss: 0.297\n",
            "[16,   750] train acc: 0.891\n",
            "[16,  1000] train loss: 0.318\n",
            "[16,  1000] train acc: 0.889\n",
            "train Loss: 0.2892644276060164, acc: 0.89668\n",
            "[16,    50] train loss: 1.974\n",
            "[16,    50] train acc: 0.621\n",
            "[16,   100] train loss: 2.070\n",
            "[16,   100] train acc: 0.606\n",
            "[16,   150] train loss: 1.834\n",
            "[16,   150] train acc: 0.624\n",
            "[16,   200] train loss: 2.005\n",
            "[16,   200] train acc: 0.603\n",
            "val Loss: 1.9707318824529647, acc: 0.6136\n",
            "******************************\n",
            "Epoch 17/20\n",
            "******************************\n",
            "[17,   250] train loss: 0.236\n",
            "[17,   250] train acc: 0.916\n",
            "[17,   500] train loss: 0.263\n",
            "[17,   500] train acc: 0.904\n",
            "[17,   750] train loss: 0.283\n",
            "[17,   750] train acc: 0.895\n",
            "[17,  1000] train loss: 0.322\n",
            "[17,  1000] train acc: 0.882\n",
            "train Loss: 0.27589727979898454, acc: 0.89936\n",
            "[17,    50] train loss: 1.922\n",
            "[17,    50] train acc: 0.619\n",
            "[17,   100] train loss: 1.952\n",
            "[17,   100] train acc: 0.603\n",
            "[17,   150] train loss: 1.991\n",
            "[17,   150] train acc: 0.627\n",
            "[17,   200] train loss: 2.003\n",
            "[17,   200] train acc: 0.612\n",
            "val Loss: 1.9670824271440506, acc: 0.6154\n",
            "******************************\n",
            "Epoch 18/20\n",
            "******************************\n",
            "[18,   250] train loss: 0.247\n",
            "[18,   250] train acc: 0.913\n",
            "[18,   500] train loss: 0.248\n",
            "[18,   500] train acc: 0.912\n",
            "[18,   750] train loss: 0.277\n",
            "[18,   750] train acc: 0.898\n",
            "[18,  1000] train loss: 0.306\n",
            "[18,  1000] train acc: 0.890\n",
            "train Loss: 0.2695239863619208, acc: 0.90338\n",
            "[18,    50] train loss: 2.120\n",
            "[18,    50] train acc: 0.607\n",
            "[18,   100] train loss: 1.924\n",
            "[18,   100] train acc: 0.625\n",
            "[18,   150] train loss: 2.028\n",
            "[18,   150] train acc: 0.610\n",
            "[18,   200] train loss: 2.129\n",
            "[18,   200] train acc: 0.604\n",
            "val Loss: 2.0501104101538656, acc: 0.6115\n",
            "******************************\n",
            "Epoch 19/20\n",
            "******************************\n",
            "[19,   250] train loss: 0.245\n",
            "[19,   250] train acc: 0.911\n",
            "[19,   500] train loss: 0.254\n",
            "[19,   500] train acc: 0.910\n",
            "[19,   750] train loss: 0.288\n",
            "[19,   750] train acc: 0.894\n",
            "[19,  1000] train loss: 0.305\n",
            "[19,  1000] train acc: 0.888\n",
            "train Loss: 0.2728093275129795, acc: 0.90094\n",
            "[19,    50] train loss: 2.033\n",
            "[19,    50] train acc: 0.613\n",
            "[19,   100] train loss: 2.042\n",
            "[19,   100] train acc: 0.609\n",
            "[19,   150] train loss: 2.067\n",
            "[19,   150] train acc: 0.607\n",
            "[19,   200] train loss: 2.056\n",
            "[19,   200] train acc: 0.629\n",
            "val Loss: 2.0495339286327363, acc: 0.6145\n",
            "******************************\n",
            "Epoch 20/20\n",
            "******************************\n",
            "[20,   250] train loss: 0.227\n",
            "[20,   250] train acc: 0.921\n",
            "[20,   500] train loss: 0.248\n",
            "[20,   500] train acc: 0.910\n",
            "[20,   750] train loss: 0.267\n",
            "[20,   750] train acc: 0.903\n",
            "[20,  1000] train loss: 0.305\n",
            "[20,  1000] train acc: 0.888\n",
            "train Loss: 0.26192658983543515, acc: 0.90532\n",
            "[20,    50] train loss: 1.987\n",
            "[20,    50] train acc: 0.626\n",
            "[20,   100] train loss: 2.098\n",
            "[20,   100] train acc: 0.599\n",
            "[20,   150] train loss: 2.078\n",
            "[20,   150] train acc: 0.604\n",
            "[20,   200] train loss: 2.056\n",
            "[20,   200] train acc: 0.605\n",
            "val Loss: 2.054712406992912, acc: 0.6083\n",
            "******************************\n",
            "best_acc is 0.6339\n",
            "trainning finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TehDHpeaocC5"
      },
      "source": [
        "def train(epoch, train_dataloader):\r\n",
        "\r\n",
        "  for i in range(epoch):\r\n",
        "    total_loss = 0\r\n",
        "    for index, input_data in enumerate(train_dataloader, 0):\r\n",
        "      # get the images and speed up by cuda\r\n",
        "      input_imgs, input_labels = input_data\r\n",
        "      input_imgs, input_labels = input_imgs.to(device), input_labels.to(device)\r\n",
        "      # set the gradients all to zero\r\n",
        "      opt.zero_grad()\r\n",
        "\r\n",
        "      # forwar & backward & optimization\r\n",
        "      # now I am not very sure about the functions of optimazation\r\n",
        "      outputs = lenet5_net(input_imgs)\r\n",
        "      loss = loss_fun(outputs, input_labels)\r\n",
        "      loss.backward()\r\n",
        "      opt.step()\r\n",
        "\r\n",
        "      total_loss += loss.item()  # transfer a tensor with one element to a number\r\n",
        "      if index % 200 == 199:\r\n",
        "        print('[%d, %5d] loss: %.3f' %\r\n",
        "                  (i + 1, index + 1, total_loss / 2000))\r\n",
        "        total_loss = 0\r\n",
        "               \r\n",
        "  print(\"trainning finished\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeRqpNmH0yNa"
      },
      "source": [
        "train(epoch=epoch, train_dataloader = train_dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5JwzZjS8CM0"
      },
      "source": [
        "### save the trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAhfXiIb8Cos"
      },
      "source": [
        "model_path = \"./lenet5_model.pth\"\r\n",
        "torch.save(lenet5_net.state_dict(),model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-dxthgs78eB"
      },
      "source": [
        "### The next part is to validate the accuracy of this model.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQ8Cd3aE7_Lt"
      },
      "source": [
        "def validation(test_dataloader, model_path):\r\n",
        "\r\n",
        "  correct_num = 0\r\n",
        "  total_num = 0\r\n",
        "  net = Net()\r\n",
        "  net = net.to(device)\r\n",
        "  net.load_state_dict(torch.load(model_path))\r\n",
        "  for index, test_data in enumerate(test_dataloader, 0):\r\n",
        "\r\n",
        "    # get the images and speed up by cuda\r\n",
        "    test_imgs, test_labels = test_data\r\n",
        "    test_imgs, test_labels = test_imgs.to(device), test_labels.to(device)\r\n",
        "\r\n",
        "    # forward\r\n",
        "    outputs = net(test_imgs)\r\n",
        "    \r\n",
        "    correct_num += (torch.max(outputs,1).indices == test_labels).sum().item()\r\n",
        "    total_num += list(test_labels.shape)[0]\r\n",
        "\r\n",
        "    accuracy = (correct_num / total_num) * 100\r\n",
        "  print(f\"the accuracy in test_data is {accuracy}%\")\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSZ8knh6-lTn",
        "outputId": "80ac89c6-2547-4a53-bfda-85944a344de9"
      },
      "source": [
        "validation(test_dataloader, model_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the accuracy in test_data is 64.66%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHIPEFyOGhsy"
      },
      "source": [
        "### The classification process using lenet5 based on CIFAR10 has been completed, and then I want to explore the visualable problem during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ilht6QxHONQ"
      },
      "source": [
        "from tensorboardX import SummaryWriter\r\n",
        "writer = SummaryWriter('./Result')   # 数据存放在这个文件夹\r\n",
        "loss = 10   # 第0层\r\n",
        "for i, (name, param) in enumerate(lenet5_net.named_parameters()):\r\n",
        "    if 'bn' not in name:\r\n",
        "        writer.add_histogram(name, param, 0)\r\n",
        "        writer.add_scalar('loss', loss, i)\r\n",
        "        loss = loss*0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nz0_XD6iBv1X"
      },
      "source": [
        "### Connect the google drive and colab. Therefore, I can save the data to the Google drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOdfVwX7_1Ld"
      },
      "source": [
        "import os\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "path = \"/content/drive/My Drive\"\r\n",
        "os.chdir(path)\r\n",
        "os.listdir(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gr3HZiiDPUX"
      },
      "source": [
        "### Visualable tools"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGcTrp8y_2Cs"
      },
      "source": [
        " # 导出网络结构图\r\n",
        " import hiddenlayer as hl\r\n",
        " vis_graph = h.build_graph(lenet5_net, torch.zeros([1, 3, 32, 32]).to(device))   # 获取绘制图像的对象\r\n",
        " vis_graph.theme = h.graph.THEMES[\"blue\"].copy()     # 指定主题颜色\r\n",
        " vis_graph.save(path+\"/demo1.png\")   # 保存图像的路径"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNxzsSq0MlVY"
      },
      "source": [
        "### This function is modified to have the ability to show the loss and accuracy curves"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLUnYTw9FSIJ"
      },
      "source": [
        "import time\r\n",
        "import hiddenlayer as hl\r\n",
        "\r\n",
        "# I am trying to add the calculation of accuracy into the code\r\n",
        "### I think the loss function in the train_process function has a few mistakes\r\n",
        "def train_process(model, epoch, loss_fun, optimizer, train_dataloader,test_dataloader):\r\n",
        "  \r\n",
        "  history_train = hl.History()\r\n",
        "  canvas_train = hl.Canvas()\r\n",
        "  history_test = hl.History()\r\n",
        "  canvas_test = hl.Canvas()\r\n",
        "\r\n",
        "  best_acc = 0.0\r\n",
        "  steps_test = 0\r\n",
        "  steps_train = 0\r\n",
        "  for i in range(epoch):\r\n",
        "\r\n",
        "    print('Epoch {}/{}'.format(i+1, epoch))\r\n",
        "    print(\"*\" * 30)\r\n",
        "    # there are two phases in train processes\r\n",
        "    for modes in [\"train\",\"val\"]:\r\n",
        "      running_loss = 0\r\n",
        "      temp_loss = 0\r\n",
        "      temp_correction = 0\r\n",
        "      running_correction = 0\r\n",
        "      if modes is \"train\":\r\n",
        "        model.train() \r\n",
        "        for index, input_data in enumerate(train_dataloader, 0):\r\n",
        "          # get the images and speed up by cuda\r\n",
        "          imgs, labels = input_data\r\n",
        "          imgs, labels = imgs.to(device), labels.to(device)\r\n",
        "          # set the gradients all to zero\r\n",
        "          opt.zero_grad() \r\n",
        "\r\n",
        "          with torch.set_grad_enabled(modes == \"train\"):\r\n",
        "            # forwar & backward & optimization\r\n",
        "            outputs = model(imgs)\r\n",
        "            loss = loss_fun(outputs, labels)\r\n",
        "            loss.backward()\r\n",
        "            opt.step()\r\n",
        "          running_loss += loss.item()  # transfer a tensor with one element to a number\r\n",
        "          temp_loss += loss.item()\r\n",
        "          running_correction += (torch.max(outputs,1).indices == labels).sum().item()\r\n",
        "          temp_correction += (torch.max(outputs,1).indices == labels).sum().item()\r\n",
        "          epoch_loss = running_loss / len(train_dataloader)\r\n",
        "          epoch_acc = float(running_correction) / len(dataset_cifar10_train)\r\n",
        "          steps_train = (index + 1) + len(train_dataloader) * i\r\n",
        "          if steps_train % 250 == 0:\r\n",
        "            # print('[%d, %5d] train loss: %.3f' % (i + 1, index + 1, temp_loss / 250))\r\n",
        "            # print('[%d, %5d] train acc: %.3f' % (i + 1, index + 1, temp_correction / (250*batch_size)))\r\n",
        "            # history_train.log(steps_train,loss = temp_loss / 250, accuracy=temp_correction / (250*batch_size))\r\n",
        "            # canvas_train.draw_plot([history_train[\"loss\"], history_train[\"accuracy\"]])   \r\n",
        "            # time.sleep(0.1)\r\n",
        "            temp_loss = 0\r\n",
        "            temp_correction = 0\r\n",
        "          # print(f\"{modes} Loss: {epoch_loss}, acc: {epoch_acc}\")\r\n",
        "\r\n",
        "      elif modes is \"val\":\r\n",
        "        model.eval()\r\n",
        "        for index, test_data in enumerate(test_dataloader, 0):\r\n",
        "          # get the images and speed up by cuda\r\n",
        "          imgs, labels = test_data\r\n",
        "          imgs, labels = imgs.to(device), labels.to(device) \r\n",
        "          # set the gradients all to zero\r\n",
        "          opt.zero_grad() \r\n",
        "          with torch.set_grad_enabled(modes == \"train\"):\r\n",
        "            # forward \r\n",
        "            outputs = model(imgs)\r\n",
        "            loss = loss_fun(outputs, labels)\r\n",
        "          running_loss += loss.item()  # transfer a tensor with one element to a number\r\n",
        "          temp_loss += loss.item()  # transfer a tensor with one element to a number\r\n",
        "          running_correction += (torch.max(outputs,1).indices == labels).sum().item()\r\n",
        "          temp_correction += (torch.max(outputs,1).indices == labels).sum().item()\r\n",
        "          epoch_loss = running_loss / len(test_dataloader)\r\n",
        "          epoch_acc = float(running_correction) / len(dataset_cifar10_test)\r\n",
        "          steps_test = (index + 1) + len(test_dataloader) * i\r\n",
        "          if steps_test % 50 == 0:\r\n",
        "            print('[%d, %5d] train loss: %.3f' % (i + 1, index + 1, temp_loss / 50))\r\n",
        "            print('[%d, %5d] train acc: %.3f' % (i + 1, index + 1, temp_correction / (50*batch_size)))\r\n",
        "            history_test.log(steps_test, loss = temp_loss / 50, accuracy=temp_correction / (50*batch_size))\r\n",
        "            canvas_test.draw_plot([history_test[\"loss\"], history_test[\"accuracy\"]])\r\n",
        "            time.sleep(0.1)\r\n",
        "            temp_loss = 0\r\n",
        "            temp_correction = 0\r\n",
        "\r\n",
        "          #print(f\"{modes} Loss: {epoch_loss}, acc: {epoch_acc}\")\r\n",
        "          if epoch_acc > best_acc:\r\n",
        "            best_acc = epoch_acc\r\n",
        "\r\n",
        "      if modes is \"train\":\r\n",
        "        print(f\"{modes} Loss: {epoch_loss}, acc: {epoch_acc}\")\r\n",
        "      elif modes is \"val\":\r\n",
        "        print(f\"{modes} Loss: {epoch_loss}, acc: {epoch_acc}\")\r\n",
        "\r\n",
        "    print(\"*\"*30)\r\n",
        "\r\n",
        "  print(f\"best_acc is {best_acc}\")          \r\n",
        "  print(\"trainning finished\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "djAhJcIDJKlS",
        "outputId": "211bfdd9-8b79-4b1e-fa6d-c8bd959e1d3e"
      },
      "source": [
        "train_process(model = lenet5_net, epoch=4, loss_fun=loss_fun, optimizer = opt, train_dataloader = train_dataloader,test_dataloader=test_dataloader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "val Loss: 2.7154924923181536, acc: 0.6033\n",
            "******************************\n",
            "best_acc is 0.6133\n",
            "trainning finished\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}